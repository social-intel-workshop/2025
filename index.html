<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>HAI 2025: Workshop on Socially Aware and Cooperative Intelligent Systems</title>
    <link rel="icon" type="image/x-icon" href="images/favicon.ico">
    <link rel="stylesheet" type="text/css"
        href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css"
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css">
    <link rel="stylesheet" type="text/css" href="css/main.css">
</head>

<body>
    <!--div id="year-header">
        <ul>
            <li><a href="/2025/">2025</a></li>
        </ul>
    </div-->
    <nav class="navbar navbar-expand-lg navbar-light bg-light sticky-top shadow">
        <!-- <div class="navbar-title"><a href="#">ICRA 2025</a></div> -->
        <a class="navbar-title d-none d-md-block" href="#">
            <img src="images/conference_logo.png" height="60">
        </a>
        <div class="container">
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false"
                aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse flex-grow-0" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item">
                        <a class="nav-link" href="#">Home</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#submissions">Call for Papers</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#review">Review Timeline</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#schedule">Program</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#speakers">Speakers</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#motivation">Motivation</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#organizers">Organizers</a>
                    </li>
                </ul>
            </div>
        </div>
        <a class="navbar-right-log d-none d-xl-block" href="https://www.jp.honda-ri.com/en/" target="_blank">
            <img src="images/hrijp_log.png" width="140" height="50">
        </a>
    </nav>
    <div class="container" style="max-width: 960px; margin-top: 120px;">
        <div class="jumbotron">
            <h1 class="anchor">Workshop on Socially Aware and Cooperative Intelligent Systems</h1>
            <div class="row">
                <div class="col-lg-6 col-xs-9">
                    <img src="images/banner_1.png" class="d-none d-lg-block"
                        style="width:95%; margin-top: 20px; margin-left: 20px; max-height: 200px;" alt="banner_1">
                    <!--img src="images/banner.png" class="d-none d-md-block d-lg-none d-xl-none"
                        style="margin-left: 20px; width: 95%;"-->
                    <div class="banner-info  shadow">
                        <div class="info-entry">
                            <div class="row">
                                <div class="col-md-1" style="margin-right: 8px;">
                                    <i class="fa fa-location-dot" style="font-size: 2.7rem; margin-top: 10px;"></i>
                                </div>
                                <div class="col-md-9" style="margin-top: auto; margin-bottom: auto;">
                                    Keio University Hiyoshi Campus in Yokohama, Japan
                                </div>
                            </div>
                        </div>
                        <div class="row">
                            <div class="col-md-6 info-entry">
                                <i class="fa fa-calendar">&nbsp;</i>
                                Nov 10th
                            </div>
                            <!--div class="col-md-6 info-entry">
                                <i class="fa fa-clock">&nbsp;</i>
                                08:30 to 12:30
                            </div-->
                        </div>
                    </div>
                </div>
                <div class="col-lg-6">
                    <img src="images/banner_2.png" class="d-none d-lg-block" alt="banner_2">
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-xs-12">
                <a class="anchor" id="topics"></a>
                <h2 class="h1-bullet">About the Workshop</h2>
            </div>
        </div>
        <div class="row mb-4">
            <div class="col-xs-12">
                <p style="text-indent: 50px;">This workshop proposal focuses on realizing socially
                    aware systems in the wild via cooperative intelligence by keeping humans-in-the-loop. Specifically,
                    this workshop is dedicated to discussing computational methods for sensing and recognition of
                    nonverbal cues and internal states in the wild to realize cooperative intelligence between humans
                    and intelligent systems, including learning and behavior generation complying to the social norm,
                    and other relevant technologies like social interaction datasets.
                </p>
                <p style="text-indent: 50px;">One of the main considerations to achieve cooperative intelligence between
                    humans and intelligent systems is to enable everyone and everything to know each other well, like
                    how humans can trust or infer the implicit internal states like intention, emotion, and cognitive
                    states of each other. The importance of empathy to facilitate human-robot interaction has been
                    highlighted in previous studies. However, it is difficult for intelligent systems to estimate the
                    internal states of humans because they are dependent on the complex social dynamics and environment
                    contexts. This requires intelligent systems to be capable of sensing the multi-modal inputs,
                    reasoning the underlying abstract knowledge, and generating the corresponding responses to
                    collaborate and interact with humans. There are many studies on estimating internal states of humans
                    through measurements of wearables and non-invasive sensors [10, 24], but it would be difficult to
                    implement these solutions in the wild because of the additional sensors to be worn by humans. It
                    remains an open question for intelligent systems to sense and recognize nonverbal cues and reason
                    the rich underlying internal states of humans in the wild and noisy environments. The scope of this
                    workshop includes but not limited to the following:
                </p>
                <ul class="topic-list">
                    <li>Human internal state inference, e.g., cognitive, emotional</li>
                    <li>Recognition of nonverbal cues, e.g., gaze, gesture.</li>
                    <li>Multi-modal sensing fusion for scene perception.</li>
                    <li>Learning algorithms</li>
                    <li>Generative and adversarial algorithms.</li>
                    <li>Empathetic interaction between humansintelligent systems.</li>
                    <li>Robust sensing of facial and body key points.</li>
                    <li>Personalization and trust of intelligent systems</li>
                    <li>Applications of cooperative intelligence in the wild.</li>
                </ul>
                <p class="mb-0"><b>Keywords:</b> "Socially aware AI, cooperative intelligence, group interaction, social
                    norm, nonverbal cues"
                </p>
                <!--p class="mb-0"><b>Secondary subject:</b> "Human-Robot cooperative intelligence", "Nonverbal cues
                    recognition from audiovisual", "Human internal state inference from multi-modality", "Vision
                    applications and systems", "Human-Object interaction and scene understanding"
                </p-->
            </div>
        </div>

        <div class="row">
            <div class="col-xs-12">
                <a class="anchor" id="news"></a>
                <h2 class="h1-bullet">News updates</h2>
            </div>
        </div>

        <div class="col-xs-12">
            <div class="news-table">
                <table class="table">
                    <tbody>
                        <tr>
                            <td>June 21th</td>
                            <td>Workshop webpage was launched.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="row">
            <div class="col-xs-12">
                <a class="anchor" id="submissions"></a>
                <h2 class="h1-bullet">Call for Papers</h2>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12">
                <a class="anchor" id="review"></a>
                <h4 class="sub-header">Submission Guidelines</h4>
            </div>
        </div>
        <div class="col-xs-12 content">
            <p>We invite authors to submit unpublished papers (2-4 pages excluding references) to our workshop, to be
                presented at a workshop session upon acceptance.
                Submissions will undergo a peer-review process by the workshop's program committee and accepted papers
                will be invited
                to present their works at the workshop (<a href="#presentation">see presentation format</a>). </p>
        </div>

        <div class="row" style="margin-bottom: 10px;">
            <div class="col-md-1"></div>
            <div class="col-md-1 d-none d-md-block">
                <i class="fa-solid fa-quote-left awards-quote"></i>
            </div>
            <div class="col-md-8" style="text-align: center;">
                <b>We are pleased to announce that award will be given to the best paper accepted by this workshop.</b>
            </div>
            <div class="col-md-1 d-none d-md-block">
                <i class="fa-solid fa-quote-right awards-quote"></i>
            </div>
            <div class="col-md-1"></div>
        </div>

        <div class="row">
            <div class="col-xs-12">
                <a class="anchor" id="review"></a>
                <h4 class="sub-header">Important Dates</h4>
            </div>
        </div>
        <div class="row mb-4">
            <div class="col-xs-12">
                <ul class="timeline">
                    <li>
                        <span class="date-label">June 20, 2025</span>
                        <p>Notification of workshop acceptance</p>
                    </li>
                    <li>
                        <span class="date-label">June 21, 2025</span>
                        <p>Workshop web page open</p>
                    </li>
                    <li>
                        <span class="date-label">Aug 7, 2025</span>
                        <p>HAI2025 final decisions to authors</p>
                    </li>
                    <li>
                        <span class="date-label"><s>Sep 8</s> Sep 29, 2025</span>
                        <p><span style="color: red;">FINAL EXTENSION!</span> : Workshop paper submission deadline</p>
                    </li>
                    <li>
                        <span class="date-label"><s>Oct 1</s> Oct 14, 2025</span>
                        <p>Workshop paper reviews deadline</p>
                    </li>
                    <li>
                        <span class="date-label"><s>Oct 10</s> Oct 17, 2025</span>
                        <p>Notification to authors</p>
                    </li>
                    <li>
                        <span class="date-label">Oct 31, 2025</span>
                        <p>Camera-ready deadline</p>
                    </li>
                    <li>
                        <span class="date-label">Nov 10, 2025</span>
                        <p>Workshop date</p>
                    </li>
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12">
                <a class="anchor" id="review"></a>
                <h4 class="sub-header">Submission Instructions</h4>
            </div>
        </div>
        <div class="col-xs-12" style="margin-bottom: 20px">
            Please use the IEEE conferences paper format to write your manuscript.
            <ul>
                <li>LaTex and MS Word template: <a target="_blank"
                        href="https://www.ieee.org/conferences/publishing/templates.html">https://www.ieee.org/conferences/publishing/templates.html</a>
                </li>
            </ul>
            Please submit your paper electronically through the workshop's EasyChair submission system.
            <ul>
                <li>Link to EasyChair submission system:
                    <a target="_blank"
                        href="https://easychair.org/my/conference?conf=hai2025">https://easychair.org/my/conference?conf=hai2025</a>
                </li>
            </ul>
        </div>

        <div class="row">
            <div class="col-xs-12">
                <a class="anchor" id="presentation"></a>
                <h4 class="sub-header">Presentation Format</h4>
            </div>
        </div>
        <div class="col-xs-12">
            Accepted papers should be presented in three-way presentation approach to foster active participation
            <ul class="topic-list">
                <li>Spotlight talks (8 mins talk, Q&A in the poster session) </li>
                <li>In-person A0 posters for in-depth discussions</li>
                <li>Short pre-recorded videos (about 2 minutes) to be uploaded on the workshop webpage</li>
            </ul>
        </div>

        <div class="row">
            <div class="col-xs-12">
                <a class="anchor" id="pub_format"></a>
                <h4 class="sub-header">Publication Format</h4>
            </div>
        </div>
        <div class="col-xs-12">
            Authors are recommended to archive their papers and inform workshop organizers once this procedure is
            completed. Accepted papers which have been archived will be hosted on the workshop webpage. <br>
            <ul>
                <li>Link to arXiv: <a href="https://info.arxiv.org/help/submit/index.html" target="_blank">
                        https://info.arxiv.org/help/submit/index.html</a>
                </li>
            </ul>
            Extensions of the papers presented at this workshop will be invited to submit to a special issue journal
            to-be-announced at a later date.
        </div>

        <div class="row">
            <div class="col-xs-12">
                <a class="anchor" id="schedule"></a>
                <h2 class="h1-bullet">Program</h2>
            </div>
        </div>
        <div class="row mb-4">
            <div class="col-xs-12">
                <p>We plan a full-day event for 8 hours, including oral and poster presentations of accepted/invited
                    papers,
                    talks by FOUR invited speakers, TWO interactive sessions including a demonstration and a forum
                    discussion. For participants who could not attend in person, we will disseminate the papers and
                    pre-recorded videos on our workshop page, which also consists of a comment section for Q&A.</p>
                <ul class="schelude">
                    <li>
                        <span class="time-bg">08:30</span>
                        <span class="ribbon">08:40</span>
                        <span class="schelude-item-content">Welcome and opening remarks (10 mins)</span>
                    </li>
                    <li>
                        <span class="time-bg">08:40</span>
                        <span class="ribbon">09:25</span>
                        <span class="schelude-item-content"><a href="#talk1">Invited talk I</a> (45 mins, including
                            Q&A)</span>
                    </li>
                    <li>
                        <span class="time-bg">09:25</span>
                        <span class="ribbon">10:21</span>
                        <span class="schelude-item-content"><a href="#flash-talks">Flash talks I: 7 papers</a> (8 mins
                            each)</span>
                    </li>
                    <li>
                        <span class="time-bg">10:21</span>
                        <span class="ribbon">10:55</span>
                        <span class="schelude-item-content">Coffee break and poster session I (34 mins)</span>
                    </li>
                    <li>
                        <span class="time-bg">10:55</span>
                        <span class="ribbon">11:40</span>
                        <span class="schelude-item-content"><a href="#talk2">Invited talk II</a>(45 mins, including
                            Q&A)</span>
                    </li>
                    <li>
                        <span class="time-bg">11:40</span>
                        <span class="ribbon">12:30</span>
                        <span class="schelude-item-content">Forum (50 mins)</span>
                    </li>
                    <li>
                        <span class="time-bg">13:30</span>
                        <span class="ribbon">14:15</span>
                        <span class="schelude-item-content"><a href="#talk3">Invited talk III</a> (45 mins, including
                            Q&A)</span>
                    </li>
                    <li>
                        <span class="time-bg">14:15</span>
                        <span class="ribbon">15:11</span>
                        <span class="schelude-item-content"><a href="#flash-talks">Flash talks II: 7 papers</a> (8
                            mins each)</span>
                    </li>
                    <li>
                        <span class="time-bg">15:11</span>
                        <span class="ribbon">15:45</span>
                        <span class="schelude-item-content">Coffee break and poster session II (34 mins)</span>
                    </li>
                    <li>
                        <span class="time-bg">15:45</span>
                        <span class="ribbon">16:35</span>
                        <span class="schelude-item-content">Group interaction with an agent (50 mins)</span>
                    </li>
                    <li>
                        <span class="time-bg">16:35</span>
                        <span class="ribbon">17:20</span>
                        <span class="schelude-item-content"><a href="#talk4">Invited talk IV</a>(45 mins, including
                            Q&A)</span>
                    </li>
                    <li>
                        <span class="time-bg">17:20</span>
                        <span class="ribbon">17:30</span>
                        <span class="schelude-item-content">Closing and awards ceremony (10 mins)</span>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-xs-12">
                <a class="anchor" id="speakers"></a>
                <h2 class="h1-bullet">Invited Speakers</h2>
            </div>
        </div>
        <div class="row mb-4 speaker-row">
            <div class="col-xs-12">
                <p>
                    We have confirmed the attendance of FOUR speakers:
                </p>
            </div>
            <div class="row mb-4">
                <div class="col-xl-3 mb-4" id="talk1">
                    <img src="images/speaker_1.jpg" class="img-speaker shadow" alt="img">
                </div>
                <div class="col-xl-9 mb-4">
                    <h4>Invited Talk I</h4>
                    <h3>Is This a Person Speaking? Unintended Side Effects of Simulated Humanness in Conversational AI
                    </h3>
                    <div><span>Martina Mara, </span> Johannes Kepler Universität Linz, Austria.</div>
                    <div>Link to website: <a
                            href="https://www.jku.at/en/lit-robopsychology-lab/about-us/team/martina-mara/">https://www.jku.at/en/lit-robopsychology-lab/about-us/team/martina-mara/</a>
                    </div>
                    <div class="speaker-content">
                        <b>Abstract</b>
                        <p>
                            The use of conversational AI, such as ChatGPT or DeepSeek, is rising rapidly across many
                            personal and professional settings. These AI models not only enable natural-language
                            interactions but are also increasingly equipped with realistic synthetic voices and other
                            human-like cues that mimic interpersonal communication. Research, including recent studies
                            from Prof. Mara’s Robopsychology Lab, shows that many people readily anthropomorphize AI
                            language models, attributing emotionality, personality, and intentionality to them. While
                            anthropomorphism can help reduce complexity and satisfy the human need for social
                            relatedness, it may also raise concerns. Studies suggest that individuals who more strongly
                            anthropomorphize AI are more likely to endorse granting such systems rights and to judge
                            AI-generated recommendations as more competent and trustworthy, thereby increasing the risk
                            of over-reliance on potentially flawed outputs. Although anthropomorphism of non-human
                            entities is very widespread, it varies with individual and contextual factors. Findings from
                            the Robopsychology Lab indicate that people who feel lonely or have limited AI literacy can
                            be especially prone to anthropomorphism. This talk will examine the psychological mechanisms
                            underlying AI anthropomorphism, its implications for both human-AI and human-human
                            interaction, and practical strategies for de-anthropomorphizing and improving AI literacy in
                            an increasingly AI-integrated world.
                        </p>
                        <b>Biography</b>
                        <p>
                            Martina Mara is a Full Professor of Psychology of Artificial Intelligence & Robotics and
                            Head of the Robopsychology Lab at Johannes Kepler University Linz, Austria. She earned her
                            PhD in Psychology from the University of Koblenz-Landau (2014), focusing on user acceptance
                            of highly human-like robots, and received her habilitation (venia docendi) from the
                            University of Nuremberg (2022). Trained as an empirical psychologist with an
                            interdisciplinary outlook, her research examines how people perceive, trust, and collaborate
                            with intelligent systems—and how system design can foster human autonomy, creativity, and
                            well-being.
                            Before joining JKU in 2018, Martina spent many years outside academia, working in media
                            companies and a design studio, and later at the Ars Electronica Futurelab, where she
                            collaborated with international partners, including major companies across Europe and Japan,
                            at the intersection of technology, research, and the arts. She is a co-founder of the
                            Initiative Digitalisierung Chancengerecht (IDC), serves on the board of the Austrian
                            Research Promotion Agency (FFG), and has received several honors, including the Vienna
                            Women’s Prize, a Futurezone Award, and the Kaethe Leichter Prize for outstanding
                            contributions to women’s and gender research from the Austrian Federal Ministry for Digital
                            and Economic Affairs. Alongside her teaching and research, she is passionate about science
                            communication, giving public talks, writing commentary, and creating interactive exhibitions
                            on her lab’s topics.
                        </p>
                    </div>
                </div>
            </div>
            <div class="row mb-4">
                <div class="col-xl-3 mb-4" id="talk2">
                    <img src="images/speaker_2.jpg" class="img-speaker shadow" alt="img">
                </div>
                <div class="col-xl-9 mb-4">
                    <h4>Invited Talk II</h4>
                    <h3>Constructing Human-Aware AI for Integrated Information processing</h3>
                    <div><span>Tim Schrills, </span> University of Lübeck, Germany.</div>
                    <div>Link to website:
                        <!--a
                            href="https://cpsc.yale.edu/people/marynel-vazquez">https://cpsc.yale.edu/people/marynel-vazquez</a-->
                    </div>
                    <div class="speaker-content">
                        <b>Abstract</b>
                        <!-- p>Many real-world applications require that robots engage appropriately in social interactions
                            with small groups of people. For example, a robot that serves as a tour guide may approach a
                            group of people in a museum to explain an artwork, or an educational robot may help a pair
                            of people practice a language by encouraging them to talk to each other. In this talk, I
                            will discuss lessons that we have learned over the last few years as we investigated ways to
                            enable robots to reason about social signals in these small group interactions. In
                            particular, I will discuss graph abstractions for representing social contexts, and
                            data-driven models that leverage the relational structure of interaction data to interpret
                            and generate social behavior in group human-robot interactions. Some of the work that I will
                            be presenting is a collaboration with Sarah Gillet and Iolanda Leite at KTH.
                        </p -->
                        <b>Biography</b>
                        <!-- p>
                            Marynel Vázquez is an Assistant Professor in Yale’s Computer Science Department, where she
                            leads the Interactive Machines Group. Her research focuses on advancing multi-party
                            human-robot interaction, both by studying social group phenomena and developing perception
                            and decision making algorithms that enable autonomous robot behavior. Marynel received her
                            bachelor's degree in Computer Engineering from Universidad Simón Bolívar in 2008, and
                            obtained her M.S. and Ph.D. in Robotics from Carnegie Mellon University in 2013 and 2017,
                            respectively. Before joining Yale, she was a collaborator of Disney Research and a
                            Post-Doctoral Scholar at the Stanford Vision & Learning Lab. Marynel received a 2024 AFOSR
                            YIP Award, a 2022 NSF CAREER Award, two Amazon Research Awards, and a Google Research
                            Scholar award. Her work has been recognized with best paper awards at HRI 2023 and RO-MAN
                            2022 as well as nominations for paper awards at HRI 2021, IROS 2018, and RO-MAN 2016.
                        </p -->
                    </div>
                </div>
            </div>
            <div class="row mb-4">
                <div class="col-xl-3 mb-4" id="talk3">
                    <img src="images/speaker_3.jpg" class="img-speaker shadow" alt="img">
                </div>
                <div class="col-xl-9 mb-4">
                    <h4>Invited Talk III</h4>
                    <h3>From Robot Audition to Ecological AI: Expanding Human-Agent Interaction into the Real World</h3>
                    <div><span>Kazuhiro Nakadai, </span> Institute of Science Tokyo.</div>
                    <div>Link to website: <a
                            href="https://www.ra.sc.e.titech.ac.jp/en/member/kazuhiro-nakadai/">https://www.ra.sc.e.titech.ac.jp/en/member/kazuhiro-nakadai/</a>
                    </div>
                    <div class="speaker-content">
                        <b>Abstract</b>
                        <p>
                            Human-Agent Interaction (HAI) has traditionally centered on dialogue between humans and
                            artificial agents in relatively controlled settings. In this talk, I will present a broader
                            perspective on Ecological AI—agents that adapt not only to humans, but also to the diverse
                            environments in which interaction occurs. The journey begins with robot audition, enabling
                            robots to localize, separate, and understand speech in noisy real-world acoustic scenes. We
                            then extend auditory intelligence to drone audition for disaster response, where agents must
                            support human rescuers under extreme uncertainty. Beyond audition, I will highlight efforts
                            on two-way sign language translation and generation, enabling inclusive communication that
                            bridges spoken and signed languages, and allowing agents to interact naturally with people
                            who rely on sign. Finally, I will turn to wildlife and ecosystem monitoring, where agents
                            listen to and interpret animal vocalizations, connecting humans with the voices of nature.
                            Together, these studies illustrate how HAI can evolve into human–agent–environment
                            interaction, expanding from human-centered conversation to inclusive, resilient, and
                            ecological forms of collaboration.
                        </p>
                        <b>Biography</b>
                        <p>
                            Kazuhiro Nakadai received his B.E., M.E., and Ph.D. degrees in electrical and information
                            engineering from the University of Tokyo in 1993, 1995, and 2003, respectively. He worked at
                            Nippon Telegraph and Telephone (1995–1999), the Kitano Symbiotic Systems Project, ERATO, JST
                            (1999–2003), and Honda Research Institute Japan as a principal scientist (2003–2022). He is
                            currently a professor at the Department of Systems and Control Engineering, Institute of
                            Science Tokyo (formerly Tokyo Institute of Technology). He also held visiting and special
                            appointments at Tokyo Institute of Technology (2006–2022) and served as a guest professor at
                            Waseda University (2011–2018). His research interests include artificial intelligence,
                            robotics, signal processing, computational auditory scene analysis, multimodal integration,
                            and robot audition. He served on the executive boards of the Japanese Society for Artificial
                            Intelligence (2015–2016, 2024–2025) and the Robotics Society of Japan (2017–2018). He is a
                            Fellow of IEEE and RSJ.
                        </p>
                    </div>
                </div>
            </div>
            <div class="row mb-4">
                <div class="col-xl-3 mb-4" id="talk4">
                    <img src="images/speaker_4.jpg" class="img-speaker shadow" alt="img">
                </div>
                <div class="col-xl-9 mb-4">
                    <h4>Invited Talk IV</h4>
                    <h3>The Future of Intimacy: Understanding Artificial Romantic Relationships in a Digital Age</h3>
                    <div><span>Mayu Koike,</span> Institute of Science Tokyo</div>
                    <div>Link to website:
                        <!-- a
                            href="https://researchmap.jp/inamura/?lang=english">https://researchmap.jp/inamura/?lang=english</a-->
                    </div>
                    <div class="speaker-content">
                        <b>Abstract</b>
                        <p>
                            In recent years, AI companions have gained remarkable popularity worldwide. What began as
                            relatively simple chatbots or entertainment applications has rapidly developed into systems
                            that many people now turn to for emotional support, daily conversation, and even romantic
                            companionship. For some individuals, these AI partners are not only a source of comfort but
                            also function as meaningful figures in their personal lives. This development highlights a
                            significant cultural and technological shift: artificial agents are no longer perceived
                            solely as tools, but increasingly as “partners” in human relationships.
                            The growing prevalence of AI companions prompts us to reconsider what it means to form
                            emotional attachments, and how such relationships might influence human-to-human connections
                            in broader society. This workshop presentation will examine these questions by exploring
                            recent findings on the psychology of virtual intimacy of human and virtual agents.
                            These findings highlight the psychological potential of virtual romance and the importance
                            of understanding how emerging technologies are reshaping human relationships.
                        </p>
                        <b>Biography</b>
                        <p>
                            Mayu Koike is an assistant professor at the Institute of Science Tokyo. She obtained her PhD
                            in Psychology from The University of Edinburgh. Her work focuses on the relationships
                            between people and virtual agents. She is particularly interested in how people form strong
                            attachments (love and romantic relationships) with virtual agents, and the potential for
                            improving psychological well-being within this context.
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-xs-12">
                <a class="anchor" id="flash-talks"></a>
                <h4 class="sub-header">Flash talks</h4>
            </div>
            <div id="flash-talk-1" class="poster">
                <div>
                    <span class="sub-time-bg">09:25</span>
                    <span class="sub-ribbon">09:33</span>
                </div>
                <span>title</span>
                <span>members</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/10/10_M3PT_Transformer.pdf">PDF</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/10/10_m3pt_video.webm">video</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/10/m3pt_icra_workshop_poster.pdf">poster</a>
                </div>
            </div>
            <!-- div id="flash-talk-2" class="poster">
                <div>
                    <span class="sub-time-bg">09:18</span>
                    <span class="sub-ribbon">09:23</span>
                </div>
                <span>Legged Robot Agility Guided by Human Gesture</span>
                <span>Taerim Yoon; Dongho Kang; Jin Cheng; Minsung Ahn; Stelian Coros and Sungjoon Choi</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/11/11_Interactive_Parkour_workshop_final.pdf">PDF</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/11/11_Parkour_1080.mp4">video</a>
                </div>
            </div>
            <div id="flash-talk-3" class="poster">
                <div>
                    <span class="sub-time-bg">09:23</span>
                    <span class="sub-ribbon">09:28</span>
                </div>
                <span>A Multimodal Self-supervised AI Framework for Monitoring Challenge Behavior Risks in Children with
                    ASD</span>
                <span>Zhenhao Zhao; Eunsun Chung; Kyoug-Mee Chung; Michelle Crawford; Chung Hyuk Park</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/12/12_ICRA_WorkShop_camera_ready.pdf">PDF</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/12/12_video_presentation.mp4">video</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/12/A0 Poster.pdf">poster</a>
                </div>
            </div>
            <div id="flash-talk-4" class="poster">
                <div>
                    <span class="sub-time-bg">09:28</span>
                    <span class="sub-ribbon">09:33</span>
                </div>
                <span>Empathetic engagement drives nonverbal interactions between humans and a small-scale robot</span>
                <span>Jude Fogarty; Ifeoma Nwogu; Ryan St. Pierre</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/13/13_ICRA_2025_Final.pdf">PDF</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/13/13_ICRA_2025.mp4">video</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/13/ICRA_Poster_Final.pdf">poster</a>
                </div>
            </div>
            <div id="flash-talk-5" class="poster">
                <div>
                    <span class="sub-time-bg">09:33</span>
                    <span class="sub-ribbon">09:38</span>
                </div>
                <span>Towards Embedding Dynamic Personas in Interactive Robots: Masquerading Animated Social Kinematic
                    (MASK)</span>
                <span>Jeongeun Park; Taemoon Jeong; Hyeonseong Kim; Taehyun Byun; Seungyoun Shin; Keunjun Choi; Jaewoon
                    Kwon; Taeyoon Lee; Matthew Pan; Sungjoon Choi</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer" href="./resources/mask/final.pdf">PDF</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/mask/mask_2min_presentation.mp4">video</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/mask/poster.pdf">poster</a>
                </div>
            </div>
            <div id="flash-talk-6" class="poster">
                <div>
                    <span class="sub-time-bg">09:38</span>
                    <span class="sub-ribbon">09:43</span>
                </div>
                <span>GazeHTA: End-to-end Gaze Target Detection with Head-Target Association</span>
                <span>Zhi-Yi Lin, Jouh Yeong Chew, Jan van Gemert, Xucong Zhang</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/gazeHTA/2404.10718v3.pdf">PDF</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/gazeHTA/GazeHTA_supplementary.mp4">video</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/gazeHTA/GazeHTA_poster.pdf">poster</a>
                </div>
            </div>
            <div id="flash-talk-7" class="poster">
                <div>
                    <span class="sub-time-bg">09:43</span>
                    <span class="sub-ribbon">09:48</span>
                </div>
                <span>Implicit Behavioral Cues for Enhancing Trust and Comfort in Robot Social Navigation</span>
                <span>Yi Lian, J. Taery Kim, Sehoon Ha</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/implicit/9_Implicit_Behavioral_Cues_for.pdf">PDF</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/implicit/ICRA 2025 Non-Verbal Cue Workshop Video.mp4">video</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/implicit/implicit_behavioral_cues-poster.pdf">poster</a>
                </div>
            </div>
            <div id="flash-talk-8" class="poster">
                <div>
                    <span class="sub-time-bg">09:48</span>
                    <span class="sub-ribbon">09:53</span>
                </div>
                <span>Context-aware collaborative pushing of heavy objects using skeleton-based intention
                    prediction</span>
                <span>Gokhan Solak, Gustavo J. G. Lahr, Idil Ozdamar, Arash Ajoudani</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/intention/solak2025context.pdf">PDF</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/intention/humanoids24-video-compressed.mp4">video</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/intention/poster-icra-gokhan.pdf">poster</a>
                </div>
            </div>
            <div id="flash-talk-9" class="poster">
                <div>
                    <span class="sub-time-bg">09:53</span>
                    <span class="sub-ribbon">09:58</span>
                </div>
                <span>Let Me Help You! Neuro-Symbolic Short-Context Action Anticipation</span>
                <span>Sarthak Bhagat, Samuel Li, Joseph Campbell, Yaqi Xie, Katia Sycara, Simon Stepputtis</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/anticipation/RAL_2024.pdf">PDF</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/anticipation/RAL-video-ICRA.mp4">video</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/anticipation/ICRA25-Poster-Sarthak-final.pdf">poster</a>
                </div>
            </div>
            <div id="flash-talk-10" class="poster">
                <div>
                    <span class="sub-time-bg">09:58</span>
                    <span class="sub-ribbon">10:03</span>
                </div>
                <span>COLLAGE: Collaborative Human-Agent Interaction Generation using Hierarchical Latent Diffusion and
                    Language Models</span>
                <span>Divyanshu Daiya; Damon Conover; Aniket Bera</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/collage/ICRA25_4055_Final.pdf">PDF</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/collage/COLLAGE_workshop_video.mp4">video</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer"
                        href="./resources/collage/COLLAGE_ICRA_poster.pdf">poster</a>
                </div>
            </div-->
        </div>

        <div class="row">
            <div class="col-xs-12">
                <a class="anchor" id="motivation"></a>
                <h2 class="h1-bullet">Motivation and Background</h2>
            </div>
        </div>

        <div class="row">
            <div class="col-xs-12">
                <p style="text-indent: 50px;">This workshop theme centers on the development of AI agents and systems
                    that are capable of understanding, adapting to, and reacting to collaborate with humans in
                    compliance with the social norms. These systems leverage insights from social psychology, cognitive
                    science, robotics, and Al to interpret social cues, anticipate the needs of others, and coordinate
                    actions effectively within dynamic and often unpredictable contexts. We focus on embedding social
                    awareness into AI systems, leading to Cooperative Intelligence which focuses on building trust
                    and relationship between humans and intelligent systems, instead of focusing on functions to replace
                    humans. This paradigm is expected to realize a hybrid society, where humans coexist with ubiquitous
                    intelligent agents.</p>

                <p style="text-indent: 50px;">It is increasingly important for intelligent systems-such as robots,
                    virtual agents, and human-machine interfaces-to collaborate and interact seamlessly with humans
                    across diverse settings, including homes, factories, offices, and transportation systems. Achieving
                    efficient and intelligent humans-system collaboration relies on cooperative intelligence, which
                    draws on interdisciplinary research spanning robotics, AI, human-robot and human-computer
                    interaction, computer vision, and cognitive science.</p>

            </div>
        </div>

        <div class="row">
            <div class="col-xs-12">
                <a class="anchor" id="organizers"></a>
                <h2 class="h1-bullet">Organizers</h2>
            </div>
        </div>
        <div class="row mb-4">
            <div class="col-xl-4 col-md-4 mb-4">
                <div class="organizer-card shadow-sm">
                    <h5 class="card-title">Jouh Yeong Chew</h5>
                    <p class="card-text">Honda Research Institute Japan</p>
                    <a href="mailto:jouhyeong.chew@jp.honda-ri.com">jouhyeong.chew@jp.honda-ri.com</a>
                </div>
            </div>
            <div class="col-xl-4 col-md-4 mb-4">
                <div class="organizer-card shadow-sm">
                    <h5 class="card-title">Alan Sarkisian</h5>
                    <p class="card-text">Honda Research Institute Japan</p>
                    <a href="mailto:alan.sarkisian@jp.honda-ri.com">alan.sarkisian@jp.honda-ri.com</a>
                </div>
            </div>
            <div class="col-xl-4 col-md-4 mb-4">
                <div class="organizer-card shadow-sm">
                    <h5 class="card-title">Christiane Wiebel-Herboth</h5>
                    <p class="card-text">Honda Research Institute Europe</p>
                    <a href="mailto:christiane.wiebel@honda-ri.de">christiane.wiebel@honda-ri.de</a>
                </div>
            </div>
            <div class="col-xl-4 col-md-4 mb-4">
                <div class="organizer-card shadow-sm">
                    <h5 class="card-title">Christiane Attig</h5>
                    <p class="card-text">Honda Research Institute Europe</p>
                    <a href="mailto:christiane.attig@honda-ri.de">christiane.attig@honda-ri.de</a>
                </div>
            </div>
            <div class="col-xl-4 col-md-4 mb-4">
                <div class="organizer-card shadow-sm">
                    <h5 class="card-title">Zhaobo Zheng</h5>
                    <p class="card-text">Honda Research Institute USA</p>
                    <a href="mailto:zhaobo_zheng@honda-ri.com">zhaobo_zheng@honda-ri.com</a>
                </div>
            </div>
            <div class="col-xl-4 col-md-4 mb-4">
                <div class="organizer-card shadow-sm">
                    <h5 class="card-title">Shigeaki Nishina</h5>
                    <p class="card-text">Honda Research Institute Japan</p>
                    <a href="mailto:alan.sarkisian@jp.honda-ri.com">nishina@jp.honda-ri.com</a>
                </div>
            </div>
        </div>
    </div>

    <footer class="d-flex flex-wrap justify-content-between align-items-center py-3 my-4 border-top">
        <div class="col-md-8 d-flex align-items-center">
            <a target="_blank" href="https://www.jp.honda-ri.com/en/"
                class="mb-3 me-2 mb-md-0 text-muted text-decoration-none lh-1">
                <img src="images/logo_hri.png" width="50" height="20">
            </a>
            <span class="mb-3 mb-md-0 text-body-secondary">© 2025 Honda Research Institute Japan</span>
        </div>
    </footer>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.min.js"></script>
</body>

</html>